{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import codecs # for text parsing code\n",
    "import netrc\n",
    "import rasterio as rio\n",
    "import glob\n",
    "import io\n",
    "import shutil\n",
    "from subprocess import PIPE, Popen\n",
    "import subprocess\n",
    "import fcntl\n",
    "import select\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloading(file):\n",
    "    \"\"\"\n",
    "    Downloads and unzips UAVSAR images from ASF Vertex. Only tested on .GRD Interferometric Pairs. \n",
    "    Ideally for this application only pass 1 url at a time.\n",
    "    :param zip_url: url pointing at a UAVSAR .zip file\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get NASA EARTHDATA Credentials from ~/.netrc or manual input\n",
    "    try:\n",
    "        os.chmod('/home/jovyan/.netrc', 0o600) #only necessary on jupyterhub\n",
    "        (ASF_USER, account, ASF_PASS) = netrc.netrc().authenticators(\"urs.earthdata.nasa.gov\")\n",
    "    except:\n",
    "        ASF_USER = input(\"Enter Username: \")\n",
    "        ASF_PASS = getpass.getpass(\"Enter Password: \")\n",
    "        \n",
    "        \n",
    "    data_dir = '/tmp/'\n",
    "   \n",
    "    # directory for data downloads\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.chdir(data_dir)\n",
    "\n",
    "    print(f'downloading {file}...')\n",
    "    filename = os.path.basename(file)\n",
    "\n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        \n",
    "        cmd = \"wget -1 {0} --user={1} --password={2} -P {3} --progress=bar:force\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        #os.system(cmd) \n",
    "        #subprocess.call(cmd)\n",
    "        process = Popen(['wget',file,'--user={}'.format(ASF_USER),'--password={}'.format(ASF_PASS),'-P',data_dir,'--progress=bar'], stderr=subprocess.PIPE)\n",
    "        started = False\n",
    "        for line in process.stderr:\n",
    "            line = line.decode(\"utf-8\", \"replace\")\n",
    "            if started:\n",
    "                splited = line.split()\n",
    "                if len(splited) == 9:\n",
    "                    percentage = splited[6]\n",
    "                    speed = splited[7]\n",
    "                    remaining = splited[8]\n",
    "                    print(\"Downloaded {} with {} per second and {} left.\".format(percentage, speed, remaining), end='\\r')\n",
    "            elif line == os.linesep:\n",
    "                started = True\n",
    "\n",
    "        ##Should probably be a subprocess.call(cmd) - not quite sure why but that is the perfered method\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "\n",
    "    print(\"done\")\n",
    "    \n",
    "    # unzip\n",
    "\n",
    "    for file in glob.glob(\"/tmp/*.zip\"):\n",
    "        with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "            zip_ref.printdir()\n",
    "            print('Extracting all the files now...')\n",
    "            zip_ref.extractall('/tmp')\n",
    "            print(\"done\")\n",
    "    \n",
    "    return data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/silver_34715_20011-001_20016-002_0019d_s01_L090_01_int_grd.zip'\n",
    "downloading(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metric-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(folder)\n",
    "    int_file = glob.glob(os.path.join(folder, 'int.grd'))\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs separate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "uavsar_tiff_convert('/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a3_bucket_transfer(folder):\n",
    "    \"\"\"\n",
    "    transfers converted .tiff files to the a3 cloud\n",
    "    :param folder:  (filepath) to folder containing the UAVSAR .tiff and .ann files\n",
    "    :param region: (string) the region the flight is from\n",
    "    \"\"\"\n",
    "    num_tiffs = len(glob.glob(folder+ \"*.tiff\"))\n",
    "    \n",
    "    for tiff in glob.glob(folder+ \"*.tiff\"):\n",
    "        base_name = tiff.split('/')[-1]\n",
    "        \n",
    "        region = base_name.split('_')[0]\n",
    "        year = '20' + base_name.split('_')[2][0:2]\n",
    "        flight_num = base_name.split('_')[2][2:5]\n",
    "        folder_name = '{}_{}_{}/'.format(region,year,flight_num)\n",
    "        \n",
    "        tiff_folder_fp = os.path.join(folder, folder_name)\n",
    "        if not os.path.exists(tiff_folder_fp):\n",
    "            os.mkdir(tiff_folder_fp)\n",
    "        \n",
    "\n",
    "        os.replace(tiff,tiff_folder_fp+base_name )\n",
    "    cmd = 'aws s3 cp {} s3://snowex-data/uavsar-project/UAVSAR_images/{} --recursive'.format(tiff_folder_fp,folder_name)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    ###now check for upload complete (probably a smoother way to do this. This is has a danger of hanging)\n",
    "    upload_incomplete = True\n",
    "    if upload_incomplete:\n",
    "        cmd = 'aws s3 ls s3://snowex-data/uavsar-project/UAVSAR_images/{} | wc -l'.format(folder_name)\n",
    "        stream = os.popen(cmd)\n",
    "        output = stream.read()\n",
    "        if int(output) == num_tiffs:\n",
    "            upload_incomplete = False\n",
    "            print('upload complete')\n",
    "    \n",
    "    return folder_name, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/tmp/'\n",
    "a3_bucket_transfer(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "established-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder, ignore_errors = True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocational-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/tmp/'\n",
    "clear_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(zip_url, clear_temp = True):\n",
    "    zip_num = len(zip_url)\n",
    "    count = 0\n",
    "    for url in zip_url:\n",
    "        print('Starting {} of {} zips'.format(count, zip_num))\n",
    "        data_dir = downloading(url)\n",
    "        uavsar_tiff_convert(data_dir)\n",
    "        folder_name, number_uploaded = a3_bucket_transfer(data_dir)\n",
    "        if clear_temp:\n",
    "            clear_folder(data_dir)\n",
    "        print('Created folder {} with {} images'.format(folder_name, number_uploaded.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_int_grd.zip']\n",
    "main(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_mesa_txt = open('/home/jovyan/uavsar/aws_processing/grmesa_zip_fps.txt')\n",
    "content = grand_mesa_txt.read()\n",
    "grand_mesa_zips = content.split(\"\\n\")\n",
    "grand_mesa_txt.close()\n",
    "print(grand_mesa_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_int_grd.zip...\n",
      "doneloaded 99% with 104M per second and 0s left.....\n",
      "File Name                                             Modified             Size\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HH_01.cor.grd 2021-04-20 13:56:40    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HH_01.hgt.grd 2021-04-20 14:00:52    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HH_01.int.grd 2021-04-20 14:01:00    267028272\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HH_01.unw.grd 2021-04-20 14:01:12    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HV_01.cor.grd 2021-04-20 14:01:18    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HV_01.hgt.grd 2021-04-20 14:01:24    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HV_01.int.grd 2021-04-20 14:01:32    267028272\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HV_01.unw.grd 2021-04-20 14:01:44    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VH_01.cor.grd 2021-04-20 14:01:50    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VH_01.hgt.grd 2021-04-20 14:01:56    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VH_01.int.grd 2021-04-20 14:02:04    267028272\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VH_01.unw.grd 2021-04-20 14:02:16    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VV_01.cor.grd 2021-04-20 14:02:22    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VV_01.hgt.grd 2021-04-20 14:02:28    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VV_01.int.grd 2021-04-20 14:02:36    267028272\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VV_01.unw.grd 2021-04-20 14:02:48    133514136\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VH_01.ann 2021-04-20 14:02:54        29958\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HV_01.ann 2021-04-20 14:02:54        29958\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090VV_01.ann 2021-04-20 14:02:54        29958\n",
      "grmesa_27416_21019-017_21021-005_0006d_s01_L090HH_01.ann 2021-04-20 14:02:54        29958\n",
      "Extracting all the files now...\n",
      "done\n",
      "Number of Lines: 4767\n",
      "\n",
      "Number of Samples: 7002\n",
      "\n",
      "Top left lat: 39.190190520000002  \n",
      "Top left Lon: -108.301830120000005\n",
      "Number of Lines: 4767\n",
      "\n",
      "Number of Samples: 7002\n",
      "\n",
      "Top left lat: 39.190190520000002  \n",
      "Top left Lon: -108.301830120000005\n",
      "Number of Lines: 4767\n",
      "\n",
      "Number of Samples: 7002\n",
      "\n",
      "Top left lat: 39.190190520000002  \n",
      "Top left Lon: -108.301830120000005\n",
      "Number of Lines: 4767\n",
      "\n",
      "Number of Samples: 7002\n",
      "\n",
      "Top left lat: 39.190190520000002  \n",
      "Top left Lon: -108.301830120000005\n",
      ".tiffs have been created\n",
      "upload complete\n",
      "Created folder grmesa_2021_019/ with 16\n",
      " images\n",
      "downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21017-017_21019-017_0006d_s01_L090_01_int_grd.zip...\n",
      "doneloaded 99% with 92.5M per second and 0s left..t.\n",
      "File Name                                             Modified             Size\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HH_01.cor.grd 2021-04-16 00:12:08    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HH_01.hgt.grd 2021-04-16 00:16:10    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HH_01.int.grd 2021-04-16 00:16:18    267198720\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HH_01.unw.grd 2021-04-16 00:16:30    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HV_01.cor.grd 2021-04-16 00:16:34    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HV_01.hgt.grd 2021-04-16 00:16:42    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HV_01.int.grd 2021-04-16 00:16:48    267198720\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HV_01.unw.grd 2021-04-16 00:17:00    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VH_01.cor.grd 2021-04-16 00:17:06    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VH_01.hgt.grd 2021-04-16 00:17:12    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VH_01.int.grd 2021-04-16 00:17:20    267198720\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VH_01.unw.grd 2021-04-16 00:17:32    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VV_01.cor.grd 2021-04-16 00:17:38    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VV_01.hgt.grd 2021-04-16 00:17:44    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VV_01.int.grd 2021-04-16 00:17:52    267198720\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VV_01.unw.grd 2021-04-16 00:18:04    133599360\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HH_01.ann 2021-04-16 00:18:10        30004\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VH_01.ann 2021-04-16 00:18:10        30004\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090HV_01.ann 2021-04-16 00:18:10        30004\n",
      "grmesa_27416_21017-017_21019-017_0006d_s01_L090VV_01.ann 2021-04-16 00:18:10        30004\n",
      "Extracting all the files now...\n",
      "done\n",
      "Number of Lines: 4768\n",
      "\n",
      "Number of Samples: 7005\n",
      "\n",
      "Top left lat: 39.190134960000002  \n",
      "Top left Lon: -108.300441120000002\n",
      "Number of Lines: 4768\n",
      "\n",
      "Number of Samples: 7005\n",
      "\n",
      "Top left lat: 39.190134960000002  \n",
      "Top left Lon: -108.300441120000002\n",
      "Number of Lines: 4768\n",
      "\n",
      "Number of Samples: 7005\n",
      "\n",
      "Top left lat: 39.190134960000002  \n",
      "Top left Lon: -108.300441120000002\n",
      "Number of Lines: 4768\n",
      "\n",
      "Number of Samples: 7005\n",
      "\n",
      "Top left lat: 39.190134960000002  \n",
      "Top left Lon: -108.300441120000002\n",
      ".tiffs have been created\n",
      "upload complete\n",
      "Created folder grmesa_2021_017/ with 16\n",
      " images\n",
      "downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21016-002_21017-017_0007d_s01_L090_01_int_grd.zip...\n",
      "Downloaded 76% with 147M per second and 16s left....\r"
     ]
    }
   ],
   "source": [
    "main(grand_mesa_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-thickness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
