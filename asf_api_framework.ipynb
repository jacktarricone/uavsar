{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "coated-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from datetime import datetime\n",
    "import netrc\n",
    "import getpass\n",
    "from subprocess import PIPE, Popen\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import codecs\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-difference",
   "metadata": {},
   "source": [
    "### Credentials setup for EarthData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "plain-triple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Username:  pbillecocq\n",
      "Enter Password:  ···············\n"
     ]
    }
   ],
   "source": [
    "ASF_USER = input(\"Enter Username: \")\n",
    "ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-lender",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immune-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bbox_string(polygon):\n",
    "    '''\n",
    "    Builds the string to include in the ASF search request. The bbox consists of 4 comma-separated numbers: lower left longitude,latitude, and upper right longitude,latitude.\n",
    "    \n",
    "    Parmeters\n",
    "    ----------\n",
    "    polygon: shapely polygon\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    polygon_string : string\n",
    "        String to include in the ASF request\n",
    "    '''\n",
    "    points = mapping(polygon)['coordinates'][0]\n",
    "    lower_left = points[0]\n",
    "    upper_right = points[2]\n",
    "    bbox_string = f'{lower_left[0]},{lower_left[1]},{upper_right[0]},{upper_right[1]}'\n",
    "        \n",
    "    return bbox_string\n",
    "\n",
    "def search_asf(platform, processingLevel, start, end, polygon, output_format):\n",
    "    '''\n",
    "    Search the ASF platform for images given the input parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        platform : string\n",
    "            Name of the imaging platform. Defaults to UAVSAR, but a list of supported platform is available on the ASF website\n",
    "        processingLevel : string\n",
    "            Processing level of the imaging product. \n",
    "            Possible values for UAVSAR : (KMZ, PROJECTED, PAULI, PROJECTED_ML5X5, STOKES, AMPLITUDE, BROWSE, COMPLEX, DEM_TIFF, PROJECTED_ML3X3, METADATA, AMPLITUDE_GRD, INTERFEROMETRY, INTERFEROMETRY_GRD, THUMBNAIL)\n",
    "        start : datetime object\n",
    "            Start date of the search period.\n",
    "        end : datetime object\n",
    "            End date of the search period.\n",
    "        polygon : shapely polygon defining the Area of Interest,\n",
    "        output_format: string\n",
    "            Format being returned by the ASF API. Values : CSV, JSON, KML, METALINK, COUNT, DOWNLOAD, GEOJSON\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Ouputs a search file\n",
    "    '''\n",
    "    base = 'https://api.daac.asf.alaska.edu/services/search/param'\n",
    "    start_date = start.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    end_date = end.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    aoi_string = build_bbox_string(polygon)\n",
    "    payload = {\n",
    "        'platform': platform,\n",
    "        'processingLevel': processingLevel,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'bbox': aoi_string,\n",
    "        'output': output_format\n",
    "    }\n",
    "    #request_string = build_asf_search_request(platform, processingLevel, start, end, polygon)\n",
    "    # Perform actual request to the asf\n",
    "    r = requests.get(base, params=payload)\n",
    "    \n",
    "    return r.json()\n",
    "\n",
    "def download_single_product(productUrl, path):\n",
    "    '''\n",
    "    Downloading function.\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    productUrl: string\n",
    "            Product download Url\n",
    "    path: string\n",
    "            path to download product to\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    Download the file\n",
    "    '''\n",
    "    process = Popen(['wget', productUrl, f'--user={ASF_USER}', f'--password={ASF_PASS}', '-P', path, '--progress=bar'], stderr=PIPE)\n",
    "    started = False\n",
    "    for line in process.stderr:\n",
    "        line = line.decode(\"utf-8\", \"replace\")\n",
    "        if started:\n",
    "            splited = line.split()\n",
    "            if len(splited) == 9:\n",
    "                percentage = splited[6]\n",
    "                speed = splited[7]\n",
    "                remaining = splited[8]\n",
    "                print(\"Downloaded {} with {} per second and {} left.\".format(percentage, speed, remaining), end='\\r')\n",
    "        elif line == os.linesep:\n",
    "            started = True\n",
    "    print(f\"\\nDone downloading {productUrl}\")\n",
    "    \n",
    "def search_and_bulk_download(platform, processingLevel, start, end, polygon, path):\n",
    "    '''\n",
    "    Search the ASF platform for images given the input parameters. Download found products\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        platform : string\n",
    "            Name of the imaging platform. Defaults to UAVSAR, but a list of supported platform is available on the ASF website\n",
    "        processingLevel : string\n",
    "            Processing level of the imaging product. \n",
    "            Possible values for UAVSAR : (KMZ, PROJECTED, PAULI, PROJECTED_ML5X5, STOKES, AMPLITUDE, BROWSE, COMPLEX, DEM_TIFF, PROJECTED_ML3X3, METADATA, AMPLITUDE_GRD, INTERFEROMETRY, INTERFEROMETRY_GRD, THUMBNAIL)\n",
    "        start : datetime object\n",
    "            Start date of the search period.\n",
    "        end : datetime object\n",
    "            End date of the search period.\n",
    "        polygon : shapely polygon defining the Area of Interest,\n",
    "        path: string\n",
    "            Path to download product to\n",
    "        '''\n",
    "    results = search_asf(platform, processingLevel, start, end, polygon, output_format='JSON')[0]\n",
    "    print(f'{len(results)} product(s) found')\n",
    "    for result in results:\n",
    "        downloadUrl = result['downloadUrl']\n",
    "        download_single_product(downloadUrl, path)\n",
    "        print('Unzipping product')\n",
    "        with zipfile.ZipFile(path + result['fileName'], 'r') as zip_ref:\n",
    "            zip_ref.extractall(path + result['productName'])\n",
    "        print('Deleting original .zip')\n",
    "        os.remove(path + result['fileName'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-colleague",
   "metadata": {},
   "source": [
    "#### Processing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "sophisticated-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(path, verbose = False):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(path):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(path):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(path + files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(path):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], path, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(path):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], path, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(path + '*.grd') # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        print(data_path)\n",
    "        print('****')\n",
    "        print(raster_dataset)\n",
    "        print('----')\n",
    "        raster = gdal.Translate(data_path + '.tiff', raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs separate save because of the complex format\n",
    "    data_to_process = glob.glob(path, '*.int.grd') # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(path, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "noble-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x7f48066f3ae0> >\n"
     ]
    }
   ],
   "source": [
    "truc = gdal.Open('/tmp/silver_34715_20011-001_20016-002_0019d_s01_L090HH_01.cor.grd', gdal.GA_ReadOnly)\n",
    "print(truc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "classified-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "troc = gdal.Open('/tmp/UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01/GrMesa_08112_16095-004_17091-004_0313d_s01_L090HH_01.cor.grd', gdal.GA_ReadOnly)\n",
    "print(troc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "third-sherman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UA_grmesa_27416_20003-028_20017-006_0040d_s01_L090_01',\n",
       " 'UA_grmesa_27416_20003-028_20013-004_0025d_s01_L090_01',\n",
       " 'UA_grmesa_27416_20003-028_20008-004_0018d_s01_L090_01',\n",
       " 'UA_grmesa_27416_20003-028_20005-007_0011d_s01_L090_01',\n",
       " '.empty']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/tmp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-recipe",
   "metadata": {},
   "source": [
    "### Testing playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "committed-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/uavsar/toto/UA_grmesa_27416_20003-028_20005-007_0011d_s01_L090_01\n",
      "/home/jovyan/uavsar/toto/UA_grmesa_27416_20003-028_20017-006_0040d_s01_L090_01/grmesa_27416_20003-028_20017-006_0040d_s01_L090HH_01.hgt.grd\n",
      "****\n",
      "None\n",
      "----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received a NULL pointer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6bc27b00bdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jovyan/uavsar/toto/UA_grmesa_27416_20003-028_20005-007_0011d_s01_L090_01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./toto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muavsar_tiff_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/home/jovyan/uavsar/toto/{folder}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-2af6499a37b3>\u001b[0m in \u001b[0;36muavsar_tiff_convert\u001b[0;34m(path, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mraster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tiff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Gtiff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGDT_Float32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Step 5: Save the .int raster, needs separate save because of the complex format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mTranslate\u001b[0;34m(destName, srcDS, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0msrcDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m def WarpOptions(options=None, format=None,\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mTranslateInternal\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4429\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4430\u001b[0m     \u001b[0;34m\"\"\"TranslateInternal(char const * dest, Dataset dataset, GDALTranslateOptions translateOptions, GDALProgressFunc callback=0, void * callback_data=None) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4431\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4432\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGDALWarpAppOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4433\u001b[0m     \u001b[0;34m\"\"\"Proxy of C++ GDALWarpAppOptions class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Received a NULL pointer."
     ]
    }
   ],
   "source": [
    "print('/home/jovyan/uavsar/toto/UA_grmesa_27416_20003-028_20005-007_0011d_s01_L090_01')\n",
    "for folder in os.listdir('./toto'):\n",
    "    uavsar_tiff_convert(f'/home/jovyan/uavsar/toto/{folder}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "assured-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gran Mesa polygon for testing purposes\n",
    "gran_mesa_polygon = Polygon([(-108.2883, 38.882), (-108.0001, 38.882), (-108.0001, 39.0256), (-108.2883, 39.0256), (-108.2883, 38.882)])\n",
    "start_date = datetime.strptime('2017-01-10 11:00:00', '%Y-%m-%d %H:%M:%S') \n",
    "end_date = datetime.strptime('2017-02-01 11:00:00', '%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "willing-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_asf(platform='UAVSAR', processingLevel='INTERFEROMETRY_GRD', start=start_date, end=end_date, polygon=gran_mesa_polygon, output_format='JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "respective-growth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "accredited-julian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 product(s) found\n",
      "Downloaded 99% with 187M per second and 0s left....\n",
      "Done downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/GrMesa_26108_16095-005_17091-005_0313d_s01_L090_01_int_grd.zip\n",
      "Unzipping product\n",
      "Deleting original .zip\n",
      "Downloaded 99% with 14.1M per second and 0s left...\n",
      "Done downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01_int_grd.zip\n",
      "Unzipping product\n",
      "Deleting original .zip\n"
     ]
    }
   ],
   "source": [
    "search_and_bulk_download(platform='UAVSAR', processingLevel='INTERFEROMETRY_GRD', start=start_date, end=end_date, polygon=gran_mesa_polygon, path='/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-instrument",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
